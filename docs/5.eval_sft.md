# Evaluate the SFT model

Run quick checks against the merged model: short prompt generations and optional validation metrics.

## What it does
- Loads the merged model from disk
- Generates responses for the configured prompt suite
- (Optional) Computes perplexity on a provided validation set
- Writes metrics and generations to `outputs/.../eval`

## Inputs and outputs
- Input model dir: `MERGED_DIR` (e.g., `outputs/autoif_qwen25_05b_lora/merged`)
- Output dir: `EVAL_DIR` (default: `outputs/autoif_qwen25_05b_lora/eval`)
- Files written: `metrics.json` + generated outputs per prompt

## Run (defaults)
```powershell
.\run_pipeline.bat eval-sft
```

## Useful overrides
- Context length: `CUTOFF=2048`
- Custom eval output dir: `EVAL_DIR=outputs/my_run/eval`
- Optional validation perplexity: pass a path inside the container with `--val-path` using the CLI directly, e.g.:

```powershell
.\run_pipeline.bat bash
# inside the container
python3 -m src.cli.main eval-sft ^
  --model-dir=outputs/autoif_qwen25_05b_lora/merged ^
  --output-dir=outputs/autoif_qwen25_05b_lora/eval ^
  --cutoff-len=2048 ^
  --val-path=prepared/alpaca_full_en/val.jsonl
```

## Troubleshooting
- Model not found: ensure the export step completed and `merged/` exists
- Slow runs: reduce `eval.cutoff_len` or prompt set size in `config.yaml`