# Evaluate the SFT model

Run quick checks against the merged model: short prompt generations and optional validation metrics.

## What it does
This stage performs a quick, local evaluation against a small prompt suite and (optionally) a validation JSONL for perplexity. When included in `orchestration.post_finetune`, it runs automatically after training, reusing the same run directory.

## Inputs and outputs
- Input model: `outputs/<run>/merged` (or `merged_awq` if you explicitly point to it)
- Output: `outputs/<run>/eval/metrics.json` and `logs/<run>/eval-sft/` logs

## Run (defaults)
```powershell
.\run_pipeline.bat eval-sft
```

Standalone runs reuse an existing merged model; if none is found the command fails fast. Pass `--run-name` or explicit `--model-dir`/`--output-dir` to select a run.

- Optional validation perplexity: pass a path inside the container with `--val-path` using the CLI directly, e.g.:

```powershell
.\run_pipeline.bat bash
# inside the container
python3 -m src.cli.main eval-sft ^
  --model-dir=outputs/autoif_qwen25_05b_lora/merged ^
  --output-dir=outputs/autoif_qwen25_05b_lora/eval ^
  --cutoff-len=2048 ^
  --val-path=prepared/alpaca_full_en/val.jsonl
```

## Troubleshooting
- Model not found: ensure the export step completed and `merged/` exists
- Slow runs: reduce `eval.cutoff_len` or prompt set size in `config.yaml`