# Preprocess SFT Data

Short guide for `run_pipeline.bat preprocess-sft`. The command supports two pipelines and
automatically selects the right trainer for later stages.

| Mode | Description | Outputs | Trainer |
|------|-------------|---------|---------|
| `real_drg` | Enrich raw hospital discharge letters via Oracle then build DRG classification splits. | `train.parquet`, `val.parquet`, `label2id.json`, `stats.json`, optional `train_split/`, `val_split/`. | `DRGClassificationTrainerRunner` |
| `huggingface` | Legacy Alpaca-style instruction tuning using Hugging Face datasets. | `train.jsonl`, `val.jsonl`, metadata JSON. | `SFTTrainerRunner` |

## Prerequisites for `real_drg`

- Oracle Instant Client available locally or in the container. Set `ORACLE_INSTANT_CLIENT_PATH` or
  `preprocess.real_data.oracle.instant_client_dir`.
- Oracle credentials exported via `.env`: `ORACLE_DSN`, `ORACLE_NETWORK_ALIAS` (optional), `ORACLE_USER`,
  `ORACLE_PASSWORD`, `ORACLE_TNS_PATH`.
- Access to raw letters (`\\pc-27327\Projets\DRG-Prediction\data\Extractions\RAW-JSON-SORTIE`) or the bundled
  sample folder `data/sample_real_data`.
- Optional: metadata CSV with stay dates; leave `preprocess.real_data.metadata_csv` as `null` when not available.

## Configuration excerpt

```yaml
preprocess:
  mode: real_drg
  dataset_name: drg_letters
  real_data:
    raw_dir: "\\pc-27327\Projets\DRG-Prediction\data\Extractions\RAW-JSON-SORTIE"
    sample_dir: data/sample_real_data
    use_sample_data: false          # enable in debug_config.yaml for CI
    truncate_label_to: 4            # limit DRG code length upfront
    min_tokens: 40
    min_count: 5
    tokenizer: Qwen/Qwen2.5-0.5B    # only used for token counting
    oracle:
      enabled: true
      dsn: "${ORACLE_DSN}"
      network_alias: "${ORACLE_NETWORK_ALIAS}"
      user: "${ORACLE_USER}"
      password_env: ORACLE_PASSWORD
      tns_admin: "${ORACLE_TNS_PATH}"
```

`run_pipeline.bat preprocess-sft` will emit JSONL splits under `prepared/<dataset>/<scope>/`. The training stage switches
back to `SFTTrainerRunner`, so downstream merge, AWQ, and serving commands continue to work without modification.

See `config.yaml` for additional knobs such as `keep_prefixes`, `outside_to_other`, `copy_splits`, and
`metadata_csv`.

## Running the stage

Default (full dataset):

```powershell
./run_pipeline.bat preprocess-sft
```

Sample/debug profiles (lighter tokenizer, small splits, CPU friendly):

```powershell
./run_pipeline.bat preprocess-sft CONFIG=debug_config.yaml           # DRG letters stub mode
./run_pipeline.bat preprocess-sft CONFIG=config_debug_alpaca.yaml    # Hugging Face Alpaca sample
```

Both commands run inside Docker. Logs stream to `logs/<run>/preprocess/console.log`. Enriched JSON files sit under
`prepared/<dataset_stub>/<scope>/formatted_json/` before parquet export.

## Outputs (`real_drg`)

- `train.parquet`, `val.parquet` with `text`, `drg`, integer `label`, `stay_id`, `path`, `n_tokens`.
- `label2id.json`, `stats.json`, `meta.json` for reproducibility.
- `metadata.json` summarising Oracle enrichment and dataset stats.
- Optional `train_split/`, `val_split/` trees containing the enriched JSON inputs.
- `formatted_json/no_drg_eds_files/` holding notes without a confident `predrg_max`.

## Overrides and tips

- Toggle sample vs full corpus using `preprocess.real_data.use_sample_data` or export `REAL_USE_SAMPLE=1`.
- Restrict DRG prefixes via `preprocess.real_data.keep_prefixes`; combine with `outside_to_other` to remap others to
  `outside_label`.
- Adjust `min_tokens`, `min_count`, and `truncate_label_to` for exploratory runs.
- Capture SQL statements by setting `preprocess.real_data.oracle.debug_print_sql: true` and optionally
  `debug_sql_dir`.

## Troubleshooting

- **Oracle authentication failures**: inspect env inside the container
  (`docker compose run --rm sft env | Select-String ORACLE`).
- **Empty splits**: review `logs/<run>/preprocess/console.log` and inspect `formatted_json/no_drg_eds_files/`; relax
  filters if required.
- **Tokenizer download stalls**: switch to a lightweight public tokenizer and ensure `HF_TOKEN` is set for gated
  models.
- **Re-running after config tweaks**: delete or archive the existing prepared directory to avoid mixing artefacts.

## Hugging Face / Alpaca mode

Use when Oracle access is unavailable or for CI smoke tests.

```yaml
preprocess:
  mode: huggingface
  dataset_name: tatsu-lab/alpaca
  sample_size: 2048      # set to `full` to keep the entire dataset
  filter_langs: [en]
  test_size: 0.1
  cutoff_len: 2048
  pack_sequences: true
```


