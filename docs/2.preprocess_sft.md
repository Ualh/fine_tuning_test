# Preprocess SFT data

Convert a raw instruction dataset (e.g., Alpaca) into chat-formatted JSONL splits for training.

## What it does
- Downloads the dataset from Hugging Face Hub
- Filters by language (default: en, fr)
- Samples a subset (default: 2,000) for quick iterations
- Applies the model's chat template to create `text` fields
- Writes `train.jsonl` and `val.jsonl` to a prepared directory

## Inputs and outputs
- Input dataset: `preprocess.dataset_name` (default: `tatsu-lab/alpaca`)
- Output directory: defaults to `prepared/<dataset>_<size>` or override with `PREP_DIR`
- Files written: `train.jsonl`, `val.jsonl`, and `metadata.json`

## Run (defaults)
```powershell
.\run_pipeline.bat preprocess-sft
```

## Full run (no subsampling)
To use the full dataset, set a very large sample size so it exceeds the corpus:
```powershell
.\run_pipeline.bat preprocess-sft SAMPLE_SIZE=999999 PREP_DIR=prepared/alpaca_full_en
```

## Useful overrides
- Languages: `FILTER_LANGS=en,fr` (comma-separated)
- Sequence length: `CUTOFF=2048`
- Workers: `MAX_WORKERS=4`
- Packing: `PACK_SEQS=true` (or `false`)

You can also control parameters via `config.yaml` (preferred for repeatability) and override any of them per-run using wrapper environment variables as shown below.

Example:
```powershell
.\run_pipeline.bat preprocess-sft SAMPLE_SIZE=5000 FILTER_LANGS=en PREP_DIR=prepared/alpaca_5k_en
```

## Resume behavior
If `PREP_DIR` already contains splits, the step reuses them.

## Troubleshooting
- Empty splits: widen `FILTER_LANGS` or reduce filters
- SSL/HF errors: ensure `.env` has `HF_TOKEN`, and the provided SSL-bypass is kept
- Slow preprocessing: reduce `SAMPLE_SIZE` or increase `MAX_WORKERS`
 - Check run naming: preprocessed outputs live under `prepared/<dataset>_<size>`; training/eval artefacts go under `outputs/<model>-<dataset>-<size>-runX`.