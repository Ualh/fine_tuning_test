# Setup (Docker, environment, CPU/GPU, tokens)

This project runs fully locally using Docker. Follow these steps once to get ready.

## Prerequisites
- Windows with PowerShell
- Docker Desktop installed and running (with WSL2 engine)
- (Optional, GPU) Recent NVIDIA driver installed on host; Docker Desktop GPU support enabled
- A Hugging Face token if the base model/datasets are gated

## One-time setup
1) Create a `.env` file (for HF access):

```
HF_TOKEN=hf_XXXXXXXXXXXXXXXXXXXXXXXX
```

2) Build the training image and start the container:

```powershell
ar
.\run_pipeline.bat up
```

This brings up the training service `sft` using the configuration in `config.yaml`.

## Switch between GPU and CPU

You can run training and serving on either GPU or CPU. There are two levers: Docker resources and `config.yaml` precision flags.

### Training container (`sft` service)
- Enable GPU (recommended when available):
  - In `docker-compose.yml`, ensure the `sft` service has:
    - `gpus: all`
    - The default environment is fine. Optional GPU-related envs are already present (e.g., `PYTORCH_CUDA_ALLOC_CONF`, `TORCH_CUDA_ARCH_LIST`, `NCCL_P2P_DISABLE`).
  - In `config.yaml`, keep `train.bf16: true` (preferred on Ampere/Ada) and optionally `train.fp16: true`.

- Force CPU:
  - In `docker-compose.yml`, remove or comment out `gpus: all` under the `sft` service.
  - In `config.yaml`, set:
    - `train.bf16: false`
    - `train.fp16: false`
  - The code will automatically run on CPU when CUDA isn’t available.

Tips when VRAM is tight:
- Reduce `train.batch_size` and increase `train.gradient_accumulation` proportionally.
- Keep `train.gradient_checkpointing: true` for memory savings.

### vLLM server (`vllm-server` service)
- Enable GPU serving (default in this repo):
  - In `docker-compose.yml`, ensure the `vllm-server` service has GPU access (Docker Desktop GPU enabled) and `VLLM_TARGET_DEVICE: cuda` in its environment. The compose already sets this.

- Force CPU serving:
  - In `docker-compose.yml`, set `VLLM_TARGET_DEVICE: cpu` for the `vllm-server` service.
  - GPU device access isn’t required in this case.

Note: The `vllm-server` mounts `./outputs` as `/models`, and uses `serve.served_model_relpath` from `config.yaml` to locate the merged model (resolved to `/models/<served_model_relpath>`).

## What’s configurable (tunable) and where

Most knobs live in `config.yaml` and can be overridden on the CLI. Here’s the complete map.

### paths
- `data_root`: Optional local raw cache.
- `prepared_dir`: Where preprocessed train/val JSONL are written.
- `outputs_dir`: Base output for adapters, merged model, eval reports.
- `logs_dir`: Run directories per stage with run.log and console.log.
- `huggingface_cache`: HF cache directory (can use `${USERPROFILE}` or env-provided `HF_HOME/HF_HUB_CACHE`).
- `models_mount`: Optional external models mount (not required for local vLLM which uses `./outputs`).
- `run_metadata_file`: Pointer to the latest run folder (used by tooling and tests).

### preprocess
- `dataset_name`: HF dataset repo (e.g., `tatsu-lab/alpaca`). CLI override: `--dataset-name`.
- `sample_size`: Keep first N examples (use a large number for “all”). CLI: `--sample-size`.
- `filter_langs`: List of languages to keep (e.g., `["en", "fr"]`). CLI: `--filter-langs`.
- `test_size`: Validation split ratio (0–1). CLI: `--test-size`.
- `cutoff_len`: Max tokens per example after templating. CLI: `--cutoff-len`.
- `seed`: Random seed. CLI: `--seed`.
- `save_splits`: Persist train/val JSONL to disk.
- `max_workers`: Parallelism for preprocessing map. CLI: `--max-workers`.
- `pack_sequences`: Enable sequence packing for SFT. CLI: `--pack-sequences`.

### train
- `base_model`: HF model id (e.g., `Qwen/Qwen2-7B`). CLI: `--base-model`.
- `cutoff_len`: Max seq length during SFT. CLI: `--cutoff-len`.
- `batch_size`: Per-device train batch size. CLI: `--batch-size`.
- `gradient_accumulation`: Steps to accumulate gradients. CLI: `--gradient-accumulation`.
- `epochs`: Number of epochs. CLI: `--epochs`.
- `learning_rate`, `min_learning_rate`, `weight_decay`, `warmup_ratio`, `lr_scheduler`: Standard optimizer/scheduler knobs. CLI: `--learning-rate`, `--min-learning-rate`, `--weight-decay`, `--warmup-ratio`, `--lr-scheduler`.
- LoRA: `lora_r`, `lora_alpha`, `lora_dropout`, `lora_target_modules` (list). CLI: `--lora-r`, `--lora-alpha`, `--lora-dropout`, `--lora-targets`.
- Memory/precision: `gradient_checkpointing`, `bf16`, `fp16`. CLI: `--gradient-checkpointing`, `--bf16`, `--fp16`.
- Logging: `logging_steps`, `eval_steps`. CLI: `--logging-steps`, `--eval-steps`.
- Control run length: `max_steps` (null means full epochs). CLI: not directly exposed; set in YAML.
- Resume: `resume_from` path or `latest`. CLI: `--resume-from` on the command.

### export
- `include_adapter_weights`: Keep adapter files in merge artefacts (informational; merge saves full weights regardless).
- `resume_from`: Not typically needed here.

### eval
- `cutoff_len`, `max_new_tokens`, `temperature`, `top_p`: Generation params used for quick checks. CLI: `--cutoff-len` (others are YAML-only for now).
- `prompts`: List of objects with `instruction`, `language`, optional `max_sentences`.
- `resume_from`: Optional.

### serve
- `host`, `port`: Where vLLM is exposed on the host. Used by runtime metadata and `smoke-test`.
- `max_model_len`: Passed to vLLM as `--max-model-len` (exported to `SERVED_MODEL_MAX_LEN`).
- `served_model_name`: OpenAI-compatible name vLLM announces (e.g., `Qwen2.5-0.5B-SFT`).
- `served_model_relpath`: Relative path under `outputs/` to the merged model (e.g., `autoif_qwen25_05b_lora/merged`).
- `resume_from`: Optional.

### logging
- `console_level`, `file_level`: Log levels (INFO/DEBUG/etc.).
- `tqdm_refresh_rate`: Throttle for progress bars.

### Outside of config.yaml (compose/env)
- Docker-only toggles in `docker-compose.yml` for convenience:
  - `TORCH_CUDA_ARCH_LIST`, `NCCL_P2P_DISABLE`, `WANDB_MODE`.
  - SSL bypass envs (`CURL_CA_BUNDLE`, `REQUESTS_CA_BUNDLE`, `SSL_CERT_FILE`, `PYTHONHTTPSVERIFY`, `HF_HUB_DISABLE_SSL_VERIFY`, `GIT_SSL_NO_VERIFY`). These are already wired and also enforced in Python via `disable_ssl_verification()`.

## SSL and enterprise proxies
SSL verification is disabled by default to avoid enterprise TLS interception issues. This is handled by environment variables in `docker-compose.yml` and a runtime helper (`disable_ssl_verification()`). If you don’t need this, you can remove those toggles later.

## Start/stop containers
```powershell
.\run_pipeline.bat up     # start training container (service: sft)
.\run_pipeline.bat down   # stop all containers
.\run_pipeline.bat bash   # shell inside the training container
```