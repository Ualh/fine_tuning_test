# Setup (Docker, environment, CPU/GPU, tokens)

This project runs fully locally using Docker. Follow these steps once to get ready.

## Prerequisites
- Windows with PowerShell
- Docker Desktop installed and running (with WSL2 engine)
- (Optional, GPU) Recent NVIDIA driver installed on host; Docker Desktop GPU support enabled
- A Hugging Face token if the base model/datasets are gated

## One-time setup
1) Create a `.env` file (for HF access):

```
HF_TOKEN=hf_XXXXXXXXXXXXXXXXXXXXXXXX
```

2) Build the training image and start the container:

```powershell
.\run_pipeline.bat build
.\run_pipeline.bat up
```

This brings up the training service `sft` using the configuration in `config.yaml`.

## Switch between GPU and CPU

You can run training and serving on either GPU or CPU. There are two levers: Docker resources and `config.yaml` precision flags.

### Training container (`sft` service)
- Enable GPU (recommended when available):
  - In `docker-compose.yml`, ensure the `sft` service has:
    - `gpus: all`
    - The default environment is fine. Optional GPU-related envs are already present (e.g., `PYTORCH_CUDA_ALLOC_CONF`, `TORCH_CUDA_ARCH_LIST`, `NCCL_P2P_DISABLE`).
  - In `config.yaml`, keep `train.bf16: true` (preferred on Ampere/Ada) and optionally `train.fp16: true`.

- Force CPU:
  - In `docker-compose.yml`, remove or comment out `gpus: all` under the `sft` service.
  - In `config.yaml`, set:
    - `train.bf16: false`
    - `train.fp16: false`
  - The code will automatically run on CPU when CUDA isn’t available.

Tips when VRAM is tight:
- Reduce `train.batch_size` and increase `train.gradient_accumulation` proportionally.
- Keep `train.gradient_checkpointing: true` for memory savings.

### vLLM server (`vllm-server` service)
- Enable GPU serving (default in this repo):
  - In `docker-compose.yml`, ensure the `vllm-server` service has GPU access (Docker Desktop GPU enabled) and `VLLM_TARGET_DEVICE: cuda` in its environment. The compose already sets this.

- Force CPU serving:
  - In `docker-compose.yml`, set `VLLM_TARGET_DEVICE: cpu` for the `vllm-server` service.
  - GPU device access isn’t required in this case.

Note: The `vllm-server` mounts `./outputs` as `/models` and serves `<run>/merged_awq` when available. This is controlled by `serve.prefer_awq` and can be overridden with `serve.served_model_relpath`.

## What’s configurable (tunable) and where

Most knobs live in `config.yaml` and can be overridden on the CLI. Here’s the complete map.

### paths
- `data_root`: Optional local raw cache.
- `prepared_dir`: Where preprocessed train/val JSONL are written.
- `outputs_dir`: Base folder for per-run artefacts (`outputs/<run-name>/adapter`, `merged`, `eval`, ...). `outputs/latest.txt` points to the newest run.
- `logs_dir`: Per-run stage logs (`logs/<run-name>/<stage>/run.log`). `logs/latest.txt` mirrors the newest run root.
- `huggingface_cache`: HF cache directory (can use `${USERPROFILE}` or env-provided `HF_HOME/HF_HUB_CACHE`).
- `models_mount`: Optional external models mount (not required for local vLLM which uses `./outputs`).
- `run_metadata_file`: Pointer to the latest run folder (used by tooling and tests).

### preprocess
- `dataset_name`: HF dataset repo (e.g., `tatsu-lab/alpaca`). CLI override: `--dataset-name`.
- `sample_size`: Keep first N examples (use a large number for “all”). CLI: `--sample-size`.
- `filter_langs`: List of languages to keep (e.g., `["en", "fr"]`). CLI: `--filter-langs`.
- `test_size`: Validation split ratio (0–1). CLI: `--test-size`.
- `cutoff_len`: Max tokens per example after templating. CLI: `--cutoff-len`.
- `seed`: Random seed. CLI: `--seed`.
- `save_splits`: Persist train/val JSONL to disk.
- `max_workers`: Parallelism for preprocessing map. CLI: `--max-workers`.
- `pack_sequences`: Enable sequence packing for SFT. CLI: `--pack-sequences`.

### train
- `base_model`: HF model id (e.g., `Qwen/Qwen2-7B`). CLI: `--base-model`.
- `cutoff_len`: Max seq length during SFT. CLI: `--cutoff-len`.
- `batch_size`: Per-device train batch size. CLI: `--batch-size`.
- `gradient_accumulation`: Steps to accumulate gradients. CLI: `--gradient-accumulation`.
- `epochs`: Number of epochs. CLI: `--epochs`.
- `learning_rate`, `min_learning_rate`, `weight_decay`, `warmup_ratio`, `lr_scheduler`: Standard optimizer/scheduler knobs. CLI: `--learning-rate`, `--min-learning-rate`, `--weight-decay`, `--warmup-ratio`, `--lr-scheduler`.
- LoRA: `lora_r`, `lora_alpha`, `lora_dropout`, `lora_target_modules` (list). CLI: `--lora-r`, `--lora-alpha`, `--lora-dropout`, `--lora-targets`.
- Memory/precision: `gradient_checkpointing`, `bf16`, `fp16`. CLI: `--gradient-checkpointing`, `--bf16`, `--fp16`.
- Logging: `logging_steps`, `eval_steps`. CLI: `--logging-steps`, `--eval-steps`.
- Control run length: `max_steps` (null means full epochs). CLI: not directly exposed; set in YAML.
- Resume: `resume_from` path or `latest`. CLI: `--resume-from` on the command.

### export
- `include_adapter_weights`: Keep adapter files in merge artefacts (informational; merge saves full weights regardless).
- `resume_from`: Not typically needed here.

### eval
- `cutoff_len`, `max_new_tokens`, `temperature`, `top_p`: Generation params used for quick checks. CLI: `--cutoff-len` (others are YAML-only for now).
- `prompts`: List of objects with `instruction`, `language`, optional `max_sentences`.
- `resume_from`: Optional.

### serve
- `host`, `port`: Where vLLM is exposed on the host. Used by runtime metadata and `smoke-test`.
- `max_model_len`: Passed to vLLM as `--max-model-len` (exported to `SERVED_MODEL_MAX_LEN`).
- `served_model_name`: Default OpenAI-compatible name vLLM announces (e.g., `Qwen2.5-0.5B-SFT`).
- `model_name`: Optional override for the announced model name. Leave `null` to reuse `served_model_name`.
- `prefer_awq`: When `true`, prefer `<run>/merged_<suffix>` (typically `merged_awq`) over `<run>/merged`.
- `served_model_relpath`: Manual override relative to `outputs/`. Set to `null` to let the CLI auto-select.
- `resume_from`: Optional.

### logging
- `console_level`, `file_level`: Log levels (INFO/DEBUG/etc.).
- `tqdm_refresh_rate`: Throttle for progress bars.

### Outside of config.yaml (compose/env)
- Docker-only toggles in `docker-compose.yml` for convenience:
  - `TORCH_CUDA_ARCH_LIST`, `NCCL_P2P_DISABLE`, `WANDB_MODE`.
  - SSL bypass envs (`CURL_CA_BUNDLE`, `REQUESTS_CA_BUNDLE`, `SSL_CERT_FILE`, `PYTHONHTTPSVERIFY`, `HF_HUB_DISABLE_SSL_VERIFY`, `GIT_SSL_NO_VERIFY`). These are already wired and also enforced in Python via `disable_ssl_verification()`.

## SSL and enterprise proxies
SSL verification is disabled by default to avoid enterprise TLS interception issues. This is handled by environment variables in `docker-compose.yml` and a runtime helper (`disable_ssl_verification()`). If you don’t need this, you can remove those toggles later.

## Start/stop containers
```powershell
.\run_pipeline.bat up     # start training container (service: sft)
.\run_pipeline.bat down   # stop all containers
.\run_pipeline.bat bash   # shell inside the training container
```

## Run naming helpers

The pipeline derives a canonical run slug (`<model>-<dataset>-<size>-runX`) for every stage and writes all artefacts under `outputs/<run-name>/` and `logs/<run-name>/`. Preview the next slug without creating directories:

```powershell
.\run_pipeline.bat run-name-preview
.\run_pipeline.bat run-name-preview FORCE_RUN_INDEX=12
```

To override the slug for the next run, set environment variables before calling any stage:

- `FORCE_RUN_NAME=my-custom-run42` – fix both prefix and counter.
- `FORCE_RUN_INDEX=7` – reuse the computed prefix but force the counter.
- `LEGACY_RUN_NAME=legacy_layout` together with `USE_LEGACY_NAMING=1` – temporarily revert to the old folder naming.

These overrides are forwarded to the runtime probe (`print-runtime`) and to the containers launched by `run_pipeline.bat`.

## Debug pipeline tracing

- Toggle `logging.debug_pipeline: true` in any config file to propagate `DEBUG_PIPELINE=1` into the Docker runtime metadata. The batch wrapper uses this flag to emit extra context while keeping the probe output filtered.
- For a ready-to-run profile, invoke `.\run_pipeline.bat up CONFIG=debug_config.yaml`. This variant trims the dataset to 16 examples, disables mixed precision, and reduces batch sizes so you can validate the workflow quickly on CPU.