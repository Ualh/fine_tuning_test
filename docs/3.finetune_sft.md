# Fine-tune with LoRA (SFT)

Train LoRA adapters on the prepared chat-formatted splits.

## What it does
- Loads base model and tokenizer
- Applies LoRA modules to target layers
- Runs supervised fine-tuning with mixed precision (GPU) if available
- Saves adapters and trainer state under `outputs/...`

## Inputs and outputs
- Input data dir: `PREP_DIR` (must contain `train.jsonl` and `val.jsonl`)
- Output dir: `OUTPUT_DIR` (default: `outputs/autoif_qwen25_05b_lora`)
- Writes:
  - `outputs/.../adapter/` (adapter weights + tokenizer)
  - `outputs/.../trainer_state/` (checkpoints)
  - `outputs/.../metadata.json`

## Run (defaults)
```powershell
.\run_pipeline.bat finetune-sft
```

TensorBoard logging is enabled by default. Start the dashboard with `run_pipeline.bat tensorboard-up` and read more in `docs/8.tensorboard.md`.

## Full training (not just smoke)
By default, `config.yaml` sets `train.max_steps: 3` for a quick smoke pass. For a full epoch-based run:
1) Edit `config.yaml` and set:

```yaml
train:
  max_steps: null
```

2) Then run:
```powershell
.\run_pipeline.bat finetune-sft EPOCHS=1
```

Increase `EPOCHS`, adjust `BATCH`, `GRAD_ACCUM`, and learning-rate knobs as needed.

## Useful overrides
```powershell
.\run_pipeline.bat finetune-sft ^
  BASE_MODEL=Qwen/Qwen2.5-0.5B ^
  CUTOFF=2048 ^
  BATCH=4 GRAD_ACCUM=8 EPOCHS=1 ^
  LR=2e-5 MIN_LR=5e-6 WEIGHT_DECAY=0.01 WARMUP_RATIO=0.03 ^
  LORA_R=16 LORA_ALPHA=32 LORA_DROPOUT=0.05 ^
  LORA_TARGETS=q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj ^
  GRAD_CHECKPOINT=true BF16=true FP16=true ^
  LOG_STEPS=20 EVAL_STEPS=100 OUTPUT_DIR=outputs/autoif_qwen25_05b_lora
```

## Resume training
The script will auto-detect the latest checkpoint in `trainer_state/` and resume. You can also pass
`RESUME_FROM=outputs/.../trainer_state/checkpoint-XXX` to force a specific checkpoint.

## Troubleshooting
- OOM: lower `CUTOFF`, `BATCH`, or increase `GRAD_ACCUM`; keep packing enabled
- CPU-only: set `BF16=false FP16=false` (training will be slow)
- Gated model: ensure `.env` has a valid `HF_TOKEN`