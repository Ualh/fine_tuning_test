# Fine-tune with LoRA (classification or SFT)

`finetune-sft` trains LoRA adapters using the model specified in `train.base_model`. The training strategy depends on
`preprocess.mode`:

- `real_drg` – loads parquet splits from the DRG pipeline and trains a sequence-classification head (mirrors
  `example_code/finetune_drg.py`).
- `huggingface` – reuses the original SFT trainer on JSONL chat data (Alpaca, etc.).

## What it does
- When `preprocess.mode: real_drg`, the CLI instantiates `DRGClassificationTrainerRunner`, fine-tuning a
  sequence-classification model with LoRA adapters and weighted cross-entropy on the DRG labels.
- When `preprocess.mode: huggingface`, it falls back to `SFTTrainerRunner`, preserving the original instruction-tuning
  behaviour.

On success the wrapper can automatically chain follow-up stages (merge, AWQ, eval, serve) in the same run directory.

- Writes:
  - `outputs/.../adapter/` (adapter weights + tokenizer)
  - `outputs/.../trainer_state/` (checkpoints)
  - `outputs/.../metadata.json`

```powershell
.\run_pipeline.bat finetune-sft
```

Sample debug profiles (match the preprocess runs):

```powershell
./run_pipeline.bat finetune-sft CONFIG=debug_config.yaml            # DRG classifier LoRA
./run_pipeline.bat finetune-sft CONFIG=config_debug_alpaca.yaml     # Alpaca SFT LoRA
```

TensorBoard logging is enabled by default. Start the dashboard with `run_pipeline.bat tensorboard-up` and read more in `docs/8.tensorboard.md`.

## Full epoch-based run
By default, `config.yaml` may set a tiny `train.max_steps` for smoke tests. For a full epoch-based run:

1) Edit `config.yaml` and set:

```yaml
train:
  max_steps: null
```

2) Then run:

```powershell
.\run_pipeline.bat finetune-sft EPOCHS=1
```

Increase `EPOCHS`, adjust `BATCH`, `GRAD_ACCUM`, and learning-rate knobs as needed.

## Optional orchestration (post-finetune)
Configure `orchestration.post_finetune` in `config.yaml` to automatically run a sequence of stages after training completes, for example:

```yaml
orchestration:
  post_finetune:
    - export-merged
    - convert-awq
    - eval-sft
    - serve-vllm
```

The wrapper executes these targets sequentially using the same run so artefacts flow correctly and no stray run folders are created.

## Useful overrides
```powershell
.\run_pipeline.bat finetune-sft ^
  BASE_MODEL=Qwen/Qwen2.5-0.5B ^
  CUTOFF=2048 ^
  BATCH=4 GRAD_ACCUM=8 EPOCHS=1 ^
  LR=2e-5 MIN_LR=5e-6 WEIGHT_DECAY=0.01 WARMUP_RATIO=0.03 ^
  LORA_R=16 LORA_ALPHA=32 LORA_DROPOUT=0.05 ^
  LORA_TARGETS=q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj ^
  GRAD_CHECKPOINT=true BF16=true FP16=true ^
  LOG_STEPS=20 EVAL_STEPS=100 OUTPUT_DIR=outputs/autoif_qwen25_05b_lora
```

**Classification tip:** adjust `LORA_TARGETS` to modules that exist on your backbone. For BERT-style encoders for
example, `query,key,value` is often a better match than `q_proj/k_proj/v_proj`.

## Resume training
The script auto-detects the latest checkpoint in `trainer_state/` and resumes. You can also pass
`RESUME_FROM=outputs/.../trainer_state/checkpoint-XXX` to force a specific checkpoint.

## Troubleshooting
- OOM: lower `CUTOFF`, `BATCH`, or increase `GRAD_ACCUM`; keep packing enabled
- CPU-only: set `BF16=false FP16=false` (training will be slow)
- Gated model: ensure `.env` has a valid `HF_TOKEN`