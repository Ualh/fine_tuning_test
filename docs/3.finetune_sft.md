# Fine-tune with LoRA (SFT)

Train LoRA adapters on the prepared chat-formatted splits.

## What it does
Run LoRA fine-tuning in the training container. On success the wrapper can automatically chain follow-up stages (merge, AWQ, eval, serve) in the same run directory.

- Writes:
  - `outputs/.../adapter/` (adapter weights + tokenizer)
  - `outputs/.../trainer_state/` (checkpoints)
  - `outputs/.../metadata.json`

```powershell
.\run_pipeline.bat finetune-sft
```

TensorBoard logging is enabled by default. Start the dashboard with `run_pipeline.bat tensorboard-up` and read more in `docs/8.tensorboard.md`.

By default, `config.yaml` sets `train.max_steps: 3` for a quick smoke pass. For a full epoch-based run:
1) Edit `config.yaml` and set:


Optional orchestration:

- Configure `orchestration.post_finetune` in `config.yaml` to automatically run a sequence of stages after training completes, for example:

```yaml
orchestration:
  post_finetune:
    - export-merged
    - convert-awq
    - eval-sft
    - serve-vllm
```

The wrapper executes these targets sequentially using the same run so artefacts flow correctly and no stray run folders are created.
```yaml
train:
  max_steps: null
```

2) Then run:
```powershell
.\run_pipeline.bat finetune-sft EPOCHS=1
```

Increase `EPOCHS`, adjust `BATCH`, `GRAD_ACCUM`, and learning-rate knobs as needed.

## Useful overrides
```powershell
.\run_pipeline.bat finetune-sft ^
  BASE_MODEL=Qwen/Qwen2.5-0.5B ^
  CUTOFF=2048 ^
  BATCH=4 GRAD_ACCUM=8 EPOCHS=1 ^
  LR=2e-5 MIN_LR=5e-6 WEIGHT_DECAY=0.01 WARMUP_RATIO=0.03 ^
  LORA_R=16 LORA_ALPHA=32 LORA_DROPOUT=0.05 ^
  LORA_TARGETS=q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj ^
  GRAD_CHECKPOINT=true BF16=true FP16=true ^
  LOG_STEPS=20 EVAL_STEPS=100 OUTPUT_DIR=outputs/autoif_qwen25_05b_lora
```

## Resume training
The script will auto-detect the latest checkpoint in `trainer_state/` and resume. You can also pass
`RESUME_FROM=outputs/.../trainer_state/checkpoint-XXX` to force a specific checkpoint.

## Troubleshooting
- OOM: lower `CUTOFF`, `BATCH`, or increase `GRAD_ACCUM`; keep packing enabled
- CPU-only: set `BF16=false FP16=false` (training will be slow)
- Gated model: ensure `.env` has a valid `HF_TOKEN`