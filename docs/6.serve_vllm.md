# Serve with vLLM

Launch an OpenAI-compatible vLLM server on the merged model.

## What it does
- Starts the `vllm-server` container
- Serves the merged model via OpenAI API on `http://localhost:8080`

## Model path and volumes
The `vllm-server` expects the merged model at:
```
/models/autoif_qwen25_05b_lora/merged
```
By default, this is mounted from the host path `\\pc-27327\D\LLM`.

Options:
- Copy your local `outputs/autoif_qwen25_05b_lora/merged` to `\\pc-27327\D\LLM\autoif_qwen25_05b_lora\merged`
- Or change the `vllm-server` volume/command in `docker-compose.yml` to point directly to your `outputs/.../merged`

## Run
```powershell
.\run_pipeline.bat serve-vllm
```

## Smoke test
```powershell
python -m src.cli.main smoke-test --prompt "RÃ©sume AC215 en trois points."
```

## GPU notes (optional)
- To run vLLM on GPU, set `VLLM_TARGET_DEVICE=cuda` and ensure the container is granted GPU access.

## Troubleshooting
- 404 model: confirm the merged model folder exists at the path vLLM expects
- Slow responses on CPU: reduce `serve.max_model_len` in `config.yaml` or switch to GPU