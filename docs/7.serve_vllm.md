# Serve with vLLM

Launch an OpenAI-compatible vLLM server on the merged model.

## What it does
 Starts the `vllm-server` container. When included in `orchestration.post_finetune`, the wrapper starts vLLM automatically after evaluation using the same run’s merged model.
- Serves the merged model via OpenAI API on `http://localhost:8080`

## Model path and volumes
- `serve.prefer_awq` (default `true`) makes vLLM serve `<run>/merged_awq` whenever the compressed model exists.
	When the folder is missing or the flag is `false`, the CLI falls back to `<run>/merged`.
- `serve.served_model_relpath` overrides the auto-detected folder. Provide a path *relative to `outputs/`*
	(for example `my-run/merged_awq`). Set it to `null` to let the CLI decide dynamically.
- `serve.model_name` optionally overrides the name announced to OpenAI-compatible clients. Leave it `null` to
	reuse `serve.served_model_name`.
- Run `python -m src.cli.main print-runtime --format env` to inspect `SERVED_MODEL_RELPATH`,
	`SERVED_MODEL_PATH`, and `SERVED_MODEL_SOURCE` before launching the service.

The compose file mounts `./outputs` to `/models`. Any relative path reported by the CLI becomes available
inside the container at `/models/<relpath>`.

## Run
```powershell
run_pipeline.bat serve-vllm
```

## Smoke test
```powershell
python -m src.cli.main smoke-test --prompt "Résume AC215 en trois points."
```

## GPU notes (optional)
- To run vLLM on GPU, set `VLLM_TARGET_DEVICE=cuda` and ensure the container is granted GPU access.

## Troubleshooting
- 404 model: confirm the merged model folder exists at the path defined by `SERVED_MODEL_PATH`
- Slow responses on CPU: reduce `serve.max_model_len` in `config.yaml` or switch to GPU

## Extras
### Dozzle (container logs)
- URL: http://localhost:9999
- Shows real-time logs of all services in this compose project.

### Open WebUI (chat)
- URL: http://localhost:3000
- Preconfigured to use the internal OpenAI base URL (`http://vllm-server:8000/v1`).
- Alternatively, set OpenAI Base URL to `http://localhost:8080/v1` in Settings and use any placeholder API key if asked.