# Serve with vLLM

Launch an OpenAI-compatible vLLM server on the merged model.

## What it does
- Starts the `vllm-server` container
- Serves the merged model via OpenAI API on `http://localhost:8080`

## Model path and volumes
Set `serve.served_model_relpath` in `config.yaml` to the relative path under `outputs/` that contains the
merged weights. The CLI maps it to `/models/<relpath>` inside the container and exports it via the
`SERVED_MODEL_PATH` environment variable before starting vLLM.

- To serve directly from the shared drive `\\pc-27327\D\LLM`, copy or symlink the merged folder there and
	update `serve.served_model_relpath` accordingly.
- Run `python -m src.cli.main print-runtime --format env` if you want to inspect the resolved paths before
	launching the service.

## Run
```powershell
run_pipeline.bat serve-vllm
```

## Smoke test
```powershell
python -m src.cli.main smoke-test --prompt "RÃ©sume AC215 en trois points."
```

## GPU notes (optional)
- To run vLLM on GPU, set `VLLM_TARGET_DEVICE=cuda` and ensure the container is granted GPU access.

## Troubleshooting
- 404 model: confirm the merged model folder exists at the path defined by `SERVED_MODEL_PATH`
- Slow responses on CPU: reduce `serve.max_model_len` in `config.yaml` or switch to GPU

## Extras
### Dozzle (container logs)
- URL: http://localhost:9999
- Shows real-time logs of all services in this compose project.

### Open WebUI (chat)
- URL: http://localhost:3000
- Preconfigured to use the internal OpenAI base URL (`http://vllm-server:8000/v1`).
- Alternatively, set OpenAI Base URL to `http://localhost:8080/v1` in Settings and use any placeholder API key if asked.