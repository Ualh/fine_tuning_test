# AWQ / llm-compressor runner

This document explains the AWQ conversion runner: how to build and run the `awq-runner`, CLI usage, wrapper integration, and troubleshooting.

## Build the runner image

CPU (default):

```powershell
docker compose build awq-runner
```

GPU variant (optional): build the `gpu` target from `Dockerfile.awq-runner`.

## Run examples

Smoke import test:

```powershell
docker compose run --rm --no-deps -T awq-runner bash -lc "python3 -c 'import llmcompressor; print(\"awq ok\", llmcompressor.__version__)'"
```

Run compression (replace MERGED_REL):

```powershell
docker compose run --rm --no-deps -T awq-runner bash -lc "python3 /workspace/scripts/awq_runner.py --config '/workspace/config.yaml' --merged '/workspace/outputs/<MERGED_REL>/merged' --out '/workspace/outputs/<MERGED_REL>/merged/awq' --force"
```

With the wrapper, if `orchestration.post_finetune` includes `convert-awq`, conversion will run automatically after `finetune-sft` in the same run directory. To run it manually instead:

```powershell
run_pipeline.bat convert-awq CONFIG=config.yaml
```

## Tests

Unit tests live in `tests/awq/` and can be run inside the `sft` container:

```powershell
docker compose run --rm --no-deps -T sft bash -lc "python3 -m pytest -q tests/awq"
```

## Working settings & recorded versions

The project is tested with the embedded `awq-runner` and `llmcompressor` code found in the repository. Current known-good settings used during development and verification in this repo:

- llm-compressor version: 0.8.1 (installed in the `awq-runner` image)
- Runner entrypoint: `python3 -m src.training.awq_runner` (the runner prefers the system `llmcompressor` CLI when available, otherwise uses `python -m llmcompressor.oneshot` inline)
- Recommended AWQ config keys (in `config.yaml` / `debug_config.yaml` under `awq`):
	- enabled: true
	- gpu_enabled: true|false (depends on runner hardware)
	- method: awq
	- scheme: W4A16_ASYM (W4A16 variants for AWQ; W8A8 for RTN/GPTQ fallbacks)
	- num_calibration_samples: 128 (debug: 32)
	- calib_text_file: prepared/awq_calibration.txt (relative to project root)
	- use_smoothquant: true
	- smoothquant_strength: 0.8
	- ignore: ["lm_head"]

Example `awq` snippet (copy into `config.yaml`):

```yaml
awq:
	enabled: true
	gpu_enabled: false
	method: awq
	scheme: W4A16_ASYM
	num_calibration_samples: 128
	calib_text_file: prepared/awq_calibration.txt
	use_smoothquant: true
	smoothquant_strength: 0.8
	max_seq_length: 1024
	output_suffix: awq
	ignore:
		- lm_head
```

Verification checklist (after running `run_pipeline.bat convert-awq ...`):

- `outputs/<run>/merged_awq` exists.
- `outputs/<run>/merged_awq/metadata.json` exists and contains `"returncode": 0`.
- `logs/<run>/convert-awq/container.log` contains the llm-compressor lifecycle logs (smoothing, calibration, compressing) and no hard exception tracebacks.

Notes & troubleshooting

- If the CLI script `llmcompressor` is not present in the runner image (some llm-compressor releases omit a top-level console script), the runner will fall back to `python -m llmcompressor.oneshot` which is tested in this project.
- Ensure the merged model directory exists (the runner requires the merged checkpoint). If you see "Merged model path not found", run `run_pipeline.bat export-merged` first or point the `--merged` argument to a valid merged folder. When chained by orchestration this is already satisfied.
- If you request more calibration samples than the calibration file contains, llm-compressor will emit a warning and proceed with the available samples (see `metadata.json` for `calibration_row_count`).
