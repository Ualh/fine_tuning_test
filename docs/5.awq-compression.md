# AWQ / llm-compressor runner

This document explains the AWQ conversion runner: how to build and run the `awq-runner`, CLI usage, wrapper integration, and troubleshooting.

## Build the runner image

CPU (default):

```powershell
docker compose build awq-runner
```

GPU variant (optional): build the `gpu` target from `Dockerfile.awq-runner`.

## Run examples

Smoke import test:

```powershell
docker compose run --rm --no-deps -T awq-runner bash -lc "python3 -c 'import llmcompressor; print(\"awq ok\", llmcompressor.__version__)'"
```

Run compression (replace RUN_NAME):

```powershell
docker compose run --rm --no-deps -T awq-runner bash -lc "python3 -m src.training.awq_runner --config '/workspace/config.yaml' --merged '/workspace/outputs/<RUN_NAME>/merged' --out '/workspace/outputs/<RUN_NAME>/merged_awq' --force"
```

With the wrapper, if `orchestration.post_finetune` includes `convert-awq`, conversion will run automatically after `finetune-sft` in the same run directory. To run it manually instead:

```powershell
run_pipeline.bat convert-awq CONFIG=config.yaml
```

## Tests

Unit tests live in `tests/awq/` and can be run inside the `sft` container:

```powershell
docker compose run --rm --no-deps -T sft bash -lc "python3 -m pytest -q tests/awq"
```

## Working settings & recorded versions

The project is tested with the embedded `awq-runner` and `llmcompressor` code found in the repository. Current known-good settings used during development and verification in this repo:

- llm-compressor version: 0.8.1 (installed in the `awq-runner` image)
- Runner entrypoint: `python3 -m src.training.awq_runner` (the runner prefers the system `llmcompressor` CLI when available, otherwise uses `python -m llmcompressor.oneshot` inline)
- Recommended AWQ config keys (in `config.yaml` / `debug_config.yaml` under `awq`):
	- enabled: true
	- gpu_enabled: true|false (depends on runner hardware)
	- method: awq
	- scheme: W4A16_ASYM (W4A16 variants for AWQ; W8A8 for RTN/GPTQ fallbacks)
	- num_calibration_samples: 128 (debug: 32)
	- calib_text_file: prepared/awq_calibration.txt (relative to project root)
	- use_smoothquant: true
	- smoothquant_strength: 0.8
	- ignore: ["lm_head"]

Example `awq` snippet (copy into `config.yaml`):

```yaml
awq:
	enabled: true
	gpu_enabled: false
	method: awq
	scheme: W4A16_ASYM
	num_calibration_samples: 128
	calib_text_file: prepared/awq_calibration.txt
	use_smoothquant: true
	smoothquant_strength: 0.8
	max_seq_length: 1024
	output_suffix: awq
	ignore:
		- lm_head
```


Verification checklist (after running `run_pipeline.bat convert-awq ...`):

- `outputs/<run>/merged_awq` exists.
- `outputs/<run>/merged_awq/metadata.json` exists and contains `"returncode": 0`.
- `logs/<run>/convert-awq/container.log` contains the llm-compressor lifecycle logs (smoothing, calibration, compressing) and no hard exception tracebacks.

Recent changes (Nov 2025)
-------------------------

A brief summary of recent orchestration and tooling changes introduced in Nov 2025 which affect how `convert-awq` runs and how logs are captured:

- CLI-first conversion: The AWQ conversion stage is now routed through the project's Typer CLI command (`convert-awq`). Typical invocation from the wrapper is still:

	```powershell
	run_pipeline.bat convert-awq CONFIG=config.yaml
	```

	Under the hood this delegates to the helper in `src/cli/awq_convert.py` so the RunManager can create per-stage log directories and mirror stdout/stderr into `logs/<run>/convert-awq`.

- RunManager logging capture: When invoked through the CLI/wrapper the AWQ stage writes three main files in the run logs folder:
	- `run.log` — structured logger output created by the pipeline's logging configuration.
	- `console.log` — the stdout/stderr captured during the AWQ run (progress bars, llm-compressor info).
	- `container.log` — if the conversion runs inside the `awq-runner` container this file contains the container-side output; when Docker is unavailable the inline path writes the same lifecycle logs but `container.log` will indicate the fallback.

- Docker vs inline fallback: The conversion helper prefers invoking the `awq-runner` container (Docker). If Docker is not present or not desired, the helper will attempt a local inline invocation (`llmcompressor.oneshot`) which runs the compression inside the current environment (GPU usage depends on torch/CUDA availability). The metadata file (`outputs/<run>/merged_awq/metadata.json`) records the attempt type and return code.

- Runner image rebuild note: The `awq-runner` image used for containerized runs was recently rebuilt to include small CLI dependencies (Typer, Rich) and the `llmcompressor` package so it can be executed as an isolated sidecar. If you build the image manually, ensure the `Dockerfile.awq-runner` installs those packages.

- Host fallback caveat: If you attempt to call the Typer CLI directly on the host (outside containers), ensure your host Python environment has Typer and the required runtime packages installed; otherwise prefer running the wrapper which will call the containerized entrypoint.

- Calibration guidance: During testing some runs requested 128 calibration samples but the available `prepared/awq_calibration.txt` contained fewer examples (e.g. 20). To avoid calibration warnings and improve quantization stability, populate `prepared/awq_calibration.txt` with at least `num_calibration_samples` lines or reduce `num_calibration_samples` in your config.

Notes & troubleshooting

- If the CLI script `llmcompressor` is not present in the runner image (some llm-compressor releases omit a top-level console script), the runner will fall back to `python -m llmcompressor.oneshot` which is tested in this project.
- Ensure the merged model directory exists (the runner requires the merged checkpoint). If you see "Merged model path not found", run `run_pipeline.bat export-merged` first or point the `--merged` argument to a valid merged folder. When chained by orchestration this is already satisfied.
- If you request more calibration samples than the calibration file contains, llm-compressor will emit a warning and proceed with the available samples (see `metadata.json` for `calibration_row_count`).

## Archived AutoAWQ notes (historical)

The project previously included AutoAWQ-based examples and guidance. During a migration away from AutoAWQ we consolidated the active workflow around `llm-compressor` and an isolated `awq-runner` sidecar. The original AutoAWQ docs and examples were archived for reference. Below are the primary historical notes captured from those archived docs for convenience.

### AutoAWQ overview (archived)

AutoAWQ pushed ease of use and fast inference speed into one package. In the older documentation we included performance examples and usage notes. The library is now deprecated in this project in favour of `llm-compressor`.

Example inference speed (RTX 4090, Ryzen 9 7950X, 64 tokens) — historical numbers:

- Vicuna 7B (GEMV kernel): 198.848 tokens/s
- Mistral 7B (GEMM kernel): 156.317 tokens/s
- Mistral 7B (ExLlamaV2 kernel): 188.865 tokens/s
- Mixtral 46.7B (GEMM kernel): 93 tokens/s (2x 4090)

Warning: The AutoAWQ library is deprecated. This functionality has been adopted by the vLLM project in `llm-compressor`. For the recommended quantization workflow, use the AWQ examples in `llm-compressor` and the `awq-runner` outlined above.

### Historical installation notes (archived)

- Install (historical): `pip install autoawq`.
- Torch ABI compatibility matters: the runtime torch version must match the wheel used to build AutoAWQ.
- For AMD GPUs, AutoAWQ examples historically recommended `fuse_layers=False` and `use_exllama_v2=True`.
- On CPU, the historically recommended backend was Intel Extension for PyTorch (IPEX): `pip install intel_extension_for_pytorch` and pass `use_ipex=True` when loading quantized models.

### Archived examples and recipes (summary)

The archived `awq/examples.md` contained many usage snippets demonstrating how to build calibration datasets and call AutoAWQ APIs (e.g. `AutoAWQForCausalLM.from_pretrained`, `model.quantize(..., calib_data=...)`, `model.save_quantized(...)`). Those examples are kept in the repo archive for research but are no longer part of the active pipeline.

If you need the exact historical examples, they are preserved in the repository archive references (see `archive/docs/awq_examples_autoawq.md` in the project history). Prefer the `llm-compressor` workflow described in this document for production runs.

### Inference notes (archived)

Example usage (historical) — GPU inference with fused layers:

```python
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer, TextStreamer

quant_path = "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"

# Load model
model = AutoAWQForCausalLM.from_quantized(quant_path, fuse_layers=True)
tokenizer = AutoTokenizer.from_pretrained(quant_path, trust_remote_code=True)
streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

# Convert prompt to tokens and generate
tokens = tokenizer("[INST] Hello [/INST]", return_tensors='pt').input_ids.cuda()
generation_output = model.generate(tokens, streamer=streamer, max_new_tokens=512)
```

And (historical) vLLM example for loading AWQ models:

```python
from vllm import LLM, SamplingParams

llm = LLM(model="TheBloke/Llama-2-7b-Chat-AWQ", quantization="AWQ")
outputs = llm.generate(["Hello"], SamplingParams(temperature=0.8))
```

-- End of archived AutoAWQ notes --
