# Export merged model (LoRA ➜ full weights)

This stage composes your fine‑tuned LoRA adapter with the original base model and writes a self‑contained model folder you can load directly (e.g., with Transformers or serve via vLLM).

## What it does

Internally, the command:
- Loads the base model from Hugging Face Hub (honoring SSL bypass toggles in this repo).
- Attaches the LoRA adapter from `outputs/.../adapter` using PEFT.
- Calls `merge_and_unload()` to bake adapter deltas into the base weights.
- Saves the merged model (plus tokenizer and generation config) to `outputs/.../merged`.
- Writes a compact `metadata.json` with run details.

Implementation references:
- `src/cli/main.py` → command `export-merged`
- `src/training/lora_merge.py` → uses `PeftModel.merge_and_unload()` and saves tokenizer

## Inputs and outputs

Inputs
- Base model: `train.base_model` from `config.yaml` (default: `Qwen/Qwen2.5-0.5B`) or `--base-model` flag
- Adapter directory: typically `outputs/autoif_qwen25_05b_lora/adapter`

Outputs
- Merged directory (default in this project): `outputs/autoif_qwen25_05b_lora/merged/`
  - Includes `config.json`, `model.safetensors`, tokenizer files, `generation_config.json`, and `metadata.json`.

Standalone behaviour

- When invoked on its own, the command reuses an existing run. It infers the run from `--adapter-dir` or `--output-dir`; if neither is provided it will try the latest run. If no prior run can be found, it fails fast instead of creating an empty run folder. You can always pass `--run-name <slug>` explicitly.

Tip: If `orchestration.post_finetune` includes `export-merged`, this step runs automatically after `finetune-sft` and writes into the same run directory.

## Prerequisites
- A completed LoRA fine‑tuning run (folder `outputs/autoif_qwen25_05b_lora/adapter` exists).
- Internet access (to fetch base model) and a valid HF token if the base is gated.
- SSL is already disabled for HF downloads by the pipeline to support enterprise proxies.

## How to run

Option A — via the pipeline wrapper Docker (recommended)
1) Make sure the training container is up:


```powershell
# if not already activated
.\.venv\Scripts\activate
run_pipeline.bat build
```

```powershell
# From repo root
run_pipeline.bat up
```

2) Run export:

```powershell
run_pipeline.bat export-merged OUTPUT_DIR=outputs/autoif_qwen25_05b_lora BASE_MODEL=Qwen/Qwen2.5-0.5B
```

Option B — direct Typer CLI inside the container

```powershell
# Open a shell in the container
run_pipeline.bat bash

# Then inside the container shell
python3 -m src.cli.main export-merged ^
  --adapter-dir=outputs/autoif_qwen25_05b_lora/adapter ^
  --output-dir=outputs/autoif_qwen25_05b_lora/merged ^
  --base-model=Qwen/Qwen2.5-0.5B
```

Notes
- GPU is used if available (device_map="auto"), otherwise it will fall back to CPU.
- If you change the output directory, update any downstream steps accordingly.

## Serving with vLLM (optional)

The included `docker-compose.yml` has a `vllm-server` service expecting the merged model at:
`/models/autoif_qwen25_05b_lora/merged` (mounted to `\\pc-27327\D\LLM` on the host).

You can either:
- Copy `outputs/autoif_qwen25_05b_lora/merged` into `\\pc-27327\D\LLM\autoif_qwen25_05b_lora\merged`, or
- Adjust the `vllm-server` command/volume to point to your local `outputs/.../merged`.

Then:

```powershell
run_pipeline.bat serve-vllm
```

And smoke‑test:

```powershell
python -m src.cli.main smoke-test --prompt "Résume AC215 en trois points."
```

## Troubleshooting
- "Adapter directory not found": Ensure `outputs/.../adapter` exists (run fine‑tuning first).
- HF download or SSL errors: The repo sets env vars and calls `disable_ssl_verification()`; also confirm your HF token is set in `.env` (`HF_TOKEN`).
- Out of memory: Reduce `train.cutoff_len` and/or use CPU (slower) if GPU VRAM is limited.
