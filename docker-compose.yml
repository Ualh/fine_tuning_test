services:
  sft:
    env_file:
      - .env
    build:
      context: .
      dockerfile: Dockerfile
    working_dir: /app
    tty: true
    stdin_open: true
    gpus: all
    shm_size: "32gb"
    ipc: "host"
    ulimits:
      memlock: -1
      stack: 67108864
    environment:
      HF_TOKEN: "${HF_TOKEN:-}"
      HUGGING_FACE_HUB_TOKEN: "${HF_TOKEN:-}"
      HF_HOME: "/cache/huggingface"
      HF_HUB_CACHE: "/cache/huggingface"
      TRANSFORMERS_NO_ADVISORY_WARNINGS: "1"
      TOKENIZERS_PARALLELISM: "false"
      CURL_CA_BUNDLE: ""
      REQUESTS_CA_BUNDLE: ""
      SSL_CERT_FILE: ""
      PYTHONHTTPSVERIFY: "0"
      HF_HUB_DISABLE_SSL_VERIFY: "1"
      HF_HUB_DISABLE_XET: "1"
      HF_HUB_ENABLE_XET: "0"
      GIT_SSL_NO_VERIFY: "1"
      PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:128"
      TORCH_CUDA_ARCH_LIST: "8.9"
      NCCL_P2P_DISABLE: "1"
      WANDB_MODE: "offline"
    volumes:
      - .:/app
      - ${USERPROFILE}/.cache/huggingface:/cache/huggingface
      - ./prepared:/app/prepared
      - ./outputs:/app/outputs
      - ./logs:/app/logs

  awq-runner:
    build:
      context: .
      dockerfile: Dockerfile.awq-runner
    working_dir: /workspace
    tty: true
    stdin_open: true
    # GPU support: docker compose will honor `gpus` when the host and Docker
    # runtime support it. Use an override compose file to toggle GPU settings
    # for CI or developer runs if you prefer not to enable GPUs by default.
    gpus: all
    runtime: nvidia
    environment:
      CURL_CA_BUNDLE: ""
      REQUESTS_CA_BUNDLE: ""
      SSL_CERT_FILE: ""
      PYTHONHTTPSVERIFY: "0"
    volumes:
      - .:/workspace
      - ${USERPROFILE}/.cache/huggingface:/cache/huggingface
      - ./prepared:/workspace/prepared
      - ./outputs:/workspace/outputs
      - ./logs:/workspace/logs

  vllm-server:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    gpus: all
    privileged: true
    ipc: host
    environment:
      VLLM_CACHE_ROOT: /models/cache
      SERVED_MODEL_PATH: ${SERVED_MODEL_PATH:-/models/_unset}
      SERVED_MODEL_NAME: ${SERVED_MODEL_NAME:-Qwen2.5-0.5B-SFT}
      SERVED_MODEL_MAX_LEN: ${SERVED_MODEL_MAX_LEN:-2048}
      VLLM_TARGET_DEVICE: cuda # use GPU for serving
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      CURL_CA_BUNDLE: ""
      REQUESTS_CA_BUNDLE: ""
      SSL_CERT_FILE: ""
      PYTHONHTTPSVERIFY: "0"
      HF_HUB_DISABLE_SSL_VERIFY: "1"
      HF_HUB_DISABLE_XET: "1"
      HF_HUB_ENABLE_XET: "0"
      GIT_SSL_NO_VERIFY: "1"
    volumes:
      - "./outputs:/models"
    ports:
      - "8080:8000"
    command:
      - "--model"
      - "${SERVED_MODEL_PATH:-/models/_unset}"
      - "--served-model-name"
      - "${SERVED_MODEL_NAME:-Qwen2.5-0.5B-SFT}"
      - "--max-model-len"
      - "${SERVED_MODEL_MAX_LEN:-2048}"
    depends_on:
      - sft

  dozzle:
    image: amir20/dozzle:latest
    restart: unless-stopped
    environment:
      DOZZLE_ENABLE_ACTIONS: "true"
      DOZZLE_NO_ANALYTICS: "true"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    ports:
      - "9999:8080"

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    environment:
      WEBUI_AUTH: "true"
      OPENAI_API_BASE_URLS: http://vllm-server:8000/v1
      ENABLE_OLLAMA_API: "false"
      PORT: 3000
      OFFLINE_MODE: "0"
      HF_HUB_OFFLINE: "0"
    volumes:
      - ./open-webui:/app/backend/data
    ports:
      - "3000:3000"
    depends_on:
      - vllm-server

  tensorboard:
    build:
      context: .
      dockerfile: Dockerfile
    working_dir: /app
    command: bash -lc "tensorboard --logdir /app/logs --host 0.0.0.0 --port 6006"
    environment:
      CURL_CA_BUNDLE: ""
      REQUESTS_CA_BUNDLE: ""
      SSL_CERT_FILE: ""
      PYTHONHTTPSVERIFY: "0"
      HF_HUB_DISABLE_SSL_VERIFY: "1"
      HF_HUB_DISABLE_XET: "1"
      HF_HUB_ENABLE_XET: "0"
      GIT_SSL_NO_VERIFY: "1"
    volumes:
      - .:/app
      - ./logs:/app/logs
    ports:
      - "6006:6006"
    depends_on:
      - sft
