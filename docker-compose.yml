services:
  qwen25-05b:
    env_file:
      - .env
    build:
      context: .
      dockerfile: Dockerfile
    container_name: qwen25-05b
    working_dir: /app
    tty: true
    stdin_open: true
    gpus: all
    shm_size: "32gb"
    ipc: "host"
    ulimits:
      memlock: -1
      stack: 67108864
    environment:
      HF_TOKEN: "${HF_TOKEN:-}"
      HUGGING_FACE_HUB_TOKEN: "${HF_TOKEN:-}"
      HF_HOME: "/cache/huggingface"
      HF_HUB_CACHE: "/cache/huggingface"
      TRANSFORMERS_NO_ADVISORY_WARNINGS: "1"
      TOKENIZERS_PARALLELISM: "false"
      CURL_CA_BUNDLE: ""
      REQUESTS_CA_BUNDLE: ""
      SSL_CERT_FILE: ""
      PYTHONHTTPSVERIFY: "0"
      HF_HUB_DISABLE_SSL_VERIFY: "1"
      HF_HUB_DISABLE_XET: "1"
      HF_HUB_ENABLE_XET: "0"
      GIT_SSL_NO_VERIFY: "1"
      PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:128"
      TORCH_CUDA_ARCH_LIST: "8.9"
      NCCL_P2P_DISABLE: "1"
      WANDB_MODE: "offline"
    volumes:
      - .:/app
      - ${USERPROFILE}/.cache/huggingface:/cache/huggingface
      - ./prepared:/app/prepared
      - ./outputs:/app/outputs
      - ./logs:/app/logs

  vllm-server:
    image: vllm/vllm-openai:latest
    container_name: vllm-qwen25-05b
    runtime: nvidia
    gpus: all
    privileged: true
    ipc: host
    environment:
      VLLM_CACHE_ROOT: /models/cache
      VLLM_TARGET_DEVICE: cuda # use GPU for serving
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      CURL_CA_BUNDLE: ""
      REQUESTS_CA_BUNDLE: ""
      SSL_CERT_FILE: ""
      PYTHONHTTPSVERIFY: "0"
      HF_HUB_DISABLE_SSL_VERIFY: "1"
      HF_HUB_DISABLE_XET: "1"
      HF_HUB_ENABLE_XET: "0"
      GIT_SSL_NO_VERIFY: "1"
    volumes:
      - "\\\\pc-27327\\D\\LLM:/models"
    ports:
      - "8080:8000"
    command:
      - "--model"
      - "/models/autoif_qwen25_05b_lora/merged"
      - "--served-model-name"
      - "Qwen2.5-0.5B-SFT"
      - "--max-model-len"
      - "2048"
    depends_on:
      - qwen25-05b
